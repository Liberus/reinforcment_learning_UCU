{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1 of Reinforcement Learning course:  Multi-armed Bandits.\n",
    "\n",
    "Material is provided by Prof. Pablo Maldonado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The k-armed bandit problem\n",
    "\n",
    "The $k$ armed bandit problem can be phrased as follows:\n",
    "\n",
    "- There are $k$ possible actions, $\\{a_1, a_2, \\ldots , a_k\\}.$ Actions are sometimes called arms (hence $k-$armed).\n",
    "- At time $t = 1, 2, \\ldots ,$ an action is chosen and a reward $R_t$ is allocated.\n",
    "- Each action gives an *unknown* reward, sampled from an *unknown* probability distribution.\n",
    "- The reward depends only on the action.\n",
    "- **Goal**: Maximize total reward.\n",
    "\n",
    "These are a simpler class of reinforcement learning problems on which there is no influence from the state. Our actions have a noisy effect in the reward and our goal is to learn from these noisy signals. However, the effect of our actions on the reward is not influenced by any other externalities.\n",
    "\n",
    "Bandit algorithms are useful in a number of applications:\n",
    "\n",
    "\n",
    "*   Personalized content/news.\n",
    "*   Online advertising (A/B testing and similar). You have probably heard of A/B testing. A/B testing is a bandit algorithm with a pure exploration phase, followed by pure exploitation. Don't worry, this will be clear soon.\n",
    "   Clinical trials (experimental treatments for patients while\n",
    "    minimizing losses).\n",
    "*   Portfolio Optimization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving bandit algorithms\n",
    "\n",
    "Let's figure out how to deal with bandits before moving on to harder reinforcement learning problems.\n",
    "\n",
    "As we said for every action $a_i$, the reward might be still random, but it only\n",
    "depends on the action.\n",
    "\n",
    "\n",
    "$$ q_*(a_i) := \\mathbb E \\left[ R_t \\ | \\  A_t = a_i \\right], \\ \\ i = 1, 2, \\ldots ,k. $$\n",
    "\n",
    "\n",
    "To maximize rewards, we create some estimates of the $q_*$ function\n",
    "above, \n",
    "\n",
    "$$Q_t(a_i) := q_*(a),  \\ i = 1, 2, \\ldots k$$\n",
    "\n",
    "and use those estimates to choose the best actions. The $q_*$ function is the best possible return we could get from each action, whereas $Q_t$ represents our current estimate of the $q_*$ function based on what we just learned from the interaction. \n",
    "\n",
    "One way to learn about $q_*$ is to keep track of the average rewards.\n",
    "\n",
    "$$ Q_t(a_i) := \\frac{\\text{total reward of } a_i \\text{ before  } t}{\\text{times } a_i \\text{  was played  time } t} = \\frac{ \\sum_{j=1}^{t-1}R_j\\cdot 1_{A_j=a_i}}{\\sum_{j=1}^{t-1} 1_{A_j=a_i}}.$$\n",
    "\n",
    "By the law of large numbers, if the action is taken infinitely many times,\n",
    "\n",
    "$$ \\lim_{N_t(a_i) \\to +\\infty} Q_t(a_i) = q_*(a_i).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration vs Exploitation\n",
    "\n",
    "A dilemma that arises in many decision problems is the trade-off between exploration and exploitation. This simply means that if we spend a lot of time trying things randomly, we are risking our reward in the long term, out of low commitment to well-performing actions. Conversely, sticking to something too soon might fire back: imagine we try 2 out of 10 different arms of our bandit, and we just stick to the best of those two. We might be ignoring a much better arm if we don't get adventurous and try it! When we just commit to such action (i.e. the best action seen so far) we say we use a **greedy** policy. \n",
    "\n",
    "\n",
    "- The **greedy policy** at time $t$ is choosing actions $A_t^*$ such\n",
    "    that:\n",
    "    $$A_t^* \\  \\in \\ \\mathrm{argmax}_{i = 1, 2, \\ldots ,k} Q_t(a_i).$$\n",
    "\n",
    "- If at time $t$ we choose action $A_t$ such that $A_t = A_t^*$ is **exploiting** the action, whereas\n",
    "    $A_t \\neq A_t^*$ means **exploring**.\n",
    "\n",
    "\n",
    "One way to deal with the exploration-exploitation trade-off is to be $\\epsilon$-greedy. This means pretty much the same it meant in RL:\n",
    "\n",
    "\n",
    "- Create an **action-value** table (a Python dictionary) storing the\n",
    "    history of the payoffs obtained by each action.\n",
    "- Calculate the average payoff of each action.\n",
    "- Choose with probability $\\epsilon$ a non-optimal action.\n",
    "- Choose with probability $1-\\epsilon$ one of the greedy actions\n",
    "    (those with highest average payoff).\n",
    "- Break ties randomly.\n",
    "\n",
    "\n",
    "The choice of $\\epsilon$ is important, and we can observe different types of behaviour. \n",
    "\n",
    "The figures below ilustrate the rewards an agent gets for choosing different arms with different reward probabilities (shown in the third image from the right). The reward is 1 when you get it. So we see that the very conservative player (with small $\\epsilon$ get stuck with a single action from the beginning.\n",
    "\n",
    "\n",
    "![](images/lec5_eps001.png)\n",
    "\n",
    "On the opposite behaviour, with a value of $\\epsilon=1$, we see a player that does a bit better and explores a wider range of actions (in the second picture). However, the mean reward is barely around the average. This is because he is just playing randomly.\n",
    "\n",
    "\n",
    "![](images/lec5_eps1.png)\n",
    "\n",
    "Finally, we have a more balanced agent, which gets a much higher reward in average because is able to identify the correct actions to use with time.\n",
    "\n",
    "\n",
    "![](images/lec5_eps01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other action selection methods\n",
    "\n",
    "Besides $\\epsilon$-greedy, other action selection methods can be used. We will briefly describe some of them.\n",
    "\n",
    "### Softmax action selection\n",
    "\n",
    "\n",
    "It turns out that $\\epsilon$-greedy has the (sometimes unpleasant) property of choosing a really bad action with the same probability as choosing some not-too-bad action. To avoid this, we can choose our action via a mixed strategy $\\pi_t$ (that is, randomizing), using the softmax function:\n",
    "\n",
    "$$\\pi_t(a_i):= \\frac{e^{Q_t(a_i)/\\tau}}{\\sum^k_{m=1}e^{Q_t(a_m)/\\tau}}.$$\n",
    "\n",
    "Here $\\tau$ is a parameter, called the **temperature**. In the limit, when $\\tau \\to 0$, softmax action selection behaves like $\\epsilon$-greedy.\n",
    "\n",
    "For both softmax and $\\epsilon$-greedy action selection methods, we can decrease the parameters to zero as the number of steps of the episode decreases. This process is called **annealing** and it causes the bandit\n",
    "algorithm to explore less over time.\n",
    "\n",
    "\n",
    "### Upper Confidence Bound action selection\n",
    "\n",
    "In this method, we want to be sure of the performance of an action before choosing it. Formally, regret-based estimates are used to give us upper bounds on the true action values. The action selection mechanism is:\n",
    "\n",
    "$$A_t := \\mathrm{argmax}_{i = 1, \\ldots ,k} \\left[ Q_t(a_i) + C\\sqrt{\\frac{2\\cdot \\log t}{N_t(a_i)}} \\right].$$\n",
    "\n",
    "where $C$ is an upper bound for the reward. This gives us a way to control randomness, but it comes at a price. UCB suffers from *back-pedaling*, which roughly means \"I know this is bad, but I'm not completely sure about how bad it is\". So you end up choosing bad actions because you are not completely sure of their true value. It also happens that UCB algorithms may not become strictly greedy.\n",
    "\n",
    "### Reinforcement Comparison\n",
    "\n",
    "Last but not least, a method closely related to actor-critic algorithms in reinforcement learning. The motivation behind is simple: How much reward is a good reward? Once we decide on this, we can choose our actions accordingly. The algorithm works as follows:\n",
    "\n",
    "*   Set $p_t(a_i)$ a learned **preference** for taking action $a_i$.\n",
    "*   The policy\n",
    "    $\\pi_t(a_i):= \\frac{e^{p_t(a_i)}}{\\sum^k_{m=1}e^{p_t(a_m)}}$ is a\n",
    "    mixed strategy.\n",
    "*   The preference is updated as:\n",
    "    $$p_t(a_i) = p_{t-1}(a) + \\alpha(R_{t-1}-\\bar{R}_{t-1})(1_{A_{t-1}=a_i}-\\pi_{t-1}(a_i))$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\bar{R}_{t-1} := \\frac{1}{t-1}\\sum_{j=1}^{t-1}R_j.$$ Note that this reduces to softmax selection for a particular choice of the $p$ function.\n",
    "\n",
    "## Diagnosing a bandit algorithm\n",
    "\n",
    "There are different metrics to see how a bandit algorithm is performing.\n",
    "\n",
    "*   **Method 1:** Measure how often is the best arm chosen\n",
    "    +   That sounds ok, but what if the arms have similar rewards? This metric does not really make sense.\n",
    "*   **Method 2:** Average reward at each point in time (is it really\n",
    "    getting better?)\n",
    "    +   However, this would yield bad average reward for algorithms that explore a lot. So we should be aware of this when we tweak the hyperparameters of our algorith ($\\epsilon$ in this case).\n",
    "*   **Method 3:** Cumulative rewards.\n",
    "    +   This is a \"Big-picture\" metric: we can see if the cost of exploration was worth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Let's illustrate a couple of application domains.\n",
    "\n",
    "### Clinical trials\n",
    "\n",
    "In clinical trials, the goal is to evaluate $k$ possible treatments for a disease. Patients are allocated into $k$ groups and the reward is 0 or 1, depending on success of the treatment. This is a typical example where one would rather use softmax than $\\epsilon$-greedy, because it's not the same to choose a random treatment than a second most likely to work. Also, annealing should come in, because in later stages of the trial, a greater fraction of the subjects should be assigned to better treatments.\n",
    "\n",
    "### Internet advertising\n",
    "\n",
    "Each time you visit a website the publisher must choose to display one of $k$ ads, and the reward for the publisher is 0/1 (no click/click). However, not all users are alike, and they are not alike to themselves in time. Ads should be in **context** of users' activity. For this, a different class of algorithms, called **contextual bandits** is used. A contextual bandit receives an external signal to know in which context you are, and find, for each context, the best actions for that context. This is almost a reinforcement learning problem, except that the context has no influence in the dynamics to the next state. \n",
    "\n",
    "A major issue in real life deployments is that reward functions are messy and ill-specified.An amusing example of this can be seen in https://blog.openai.com/faulty-reward-functions/, where an agent learns to maximize its reward function as the score on a videogame. However, the agent learns the periodicity in which some bonus-giving tokens are regenerated on screen, and turns over and over in circles instead of finishing the episode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercises \n",
    "\n",
    "##### The 10-arm testbed\n",
    "This is a classic benchmark for bandit algorithms.\n",
    "\n",
    "- Generate randomly 2000 tasks.\n",
    "- For each task, you will face a bandit with 10 arms, each arm $i$ has reward chosen from a normal probability distribution with mean $\\mu_i$ and variance 1. The values of $\\mu_i$ should themselves be chosen with a normal distribution with mean 0 and value 1.\n",
    "- The task will be run for 1000 episodes\n",
    "\n",
    "\n",
    "You should compare plays vs \n",
    "- number of times the optimal action was chosen.\n",
    "- average reward.\n",
    "- cumulative reward.\n",
    "\n",
    "for $\\epsilon-$ greedy policy, for different values of $\\epsilon$. \n",
    "\n",
    "As a bonus, try softmax and/or UCB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "\n",
    "k = 10\n",
    "tasks_count = 2000\n",
    "episodes_count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReward(action, means):\n",
    "    return np.random.normal(means[action], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewBandit(k):\n",
    "    return np.random.normal(0, 1, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = getNewBandit(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandits = []\n",
    "for i in range(tasks_count):\n",
    "    bandits.append(getNewBandit(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "for i in range(0,k):\n",
    "    history.append([0 ,0]) #reward sum and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(history, e=0):\n",
    "    \"\"\"Chooses best arm based on history and return it's number\"\"\"\n",
    "    \n",
    "    maxAvarageReward = -1\n",
    "    maxAvarageRewardIndex = 0\n",
    "    for i, arm in enumerate(history):\n",
    "        currentArmAvarage = arm[0]/arm[1]\n",
    "        if maxAvarageReward < currentArmAvarage:\n",
    "            maxAvarageReward = currentArmAvarage\n",
    "            maxAvarageRewardIndex = i\n",
    "            \n",
    "    if e > 0 and random.uniform(0, 1) > e:\n",
    "        newArm = random.randint(0, len(history)-1)\n",
    "        maxAvarageRewardIndex = newArm if newArm < maxAvarageRewardIndex else newArm + 1\n",
    "        \n",
    "    return maxAvarageRewardIndex\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    rewardsTable = []\n",
    "    history = pd.DataFrame()\n",
    "    \n",
    "    def __init__(self, k=10):\n",
    "        self.means = np.random.normal(0, 1, k)\n",
    "        for i in range(0,k):\n",
    "            self.rewardsTable.append([0 ,0]) #reward sum and count\n",
    "    \n",
    "    def getReward(self, arm):\n",
    "        return np.random.normal(self.means[arm], 1)\n",
    "    \n",
    "    def getOptimalAction(self):\n",
    "        maxAvarageReward = -1\n",
    "        maxAvarageRewardIndex = 0\n",
    "        for i, arm in enumerate(self.rewardsTable):\n",
    "            currentArmAvarage = 0 if arm[1] == 0 else arm[0]/arm[1]\n",
    "            if maxAvarageReward < currentArmAvarage:\n",
    "                maxAvarageReward = currentArmAvarage\n",
    "                maxAvarageRewardIndex = i\n",
    "        \n",
    "        return maxAvarageRewardIndex\n",
    "    \n",
    "    def getNextArm(self, e=0):\n",
    "        \"\"\"Chooses best arm based on history and return it's number\"\"\"\n",
    "    \n",
    "        maxAvarageRewardIndex = self.getOptimalAction()\n",
    "            \n",
    "        if e > 0 and random.uniform(0, 1) > e:\n",
    "            newArm = random.randint(0, len(self.rewardsTable)-1)\n",
    "            maxAvarageRewardIndex = newArm if newArm < maxAvarageRewardIndex else newArm + 1\n",
    "        \n",
    "        return maxAvarageRewardIndex\n",
    "    \n",
    "    def play(self, times, e = 0, save_hist = False):\n",
    "        if save_hist == True:\n",
    "                history = pd.DataFrame(columns=[\"i\",\"sum\",\"avarage\"])\n",
    "        for i in range(times):\n",
    "            arm = self.getNextArm(e)\n",
    "            reward = self.getReward(arm)\n",
    "            self.rewardsTable[arm][0] += reward\n",
    "            self.rewardsTable[arm][1] += 1\n",
    "            if save_hist == True:\n",
    "                reward_sum = 0\n",
    "                if i == 0:\n",
    "                    reward_sum = reward\n",
    "                else:\n",
    "                    reward_sum = reward + self.history.iloc[[i]]\n",
    "                reward_avarage = reward_sum / len(self.rewardsTable)\n",
    "                self.history = self.history.append([i, reward_sum, reward_avarage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "band = Bandit()\n",
    "band.play(100, 0, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([1,2,3,4])\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
